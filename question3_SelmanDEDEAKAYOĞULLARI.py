# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12AUX-JaNDjOL25eoNUhLyUpHXxsoOCMB
"""

# MNIST Digit Generator Training Script
# Run this in Google Colab with T4 GPU

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import os

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Hyperparameters
batch_size = 128
learning_rate = 0.0002
num_epochs = 100
latent_dim = 100
img_size = 28

# Data preprocessing
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]
])

# Load MNIST dataset
train_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    download=True
)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True
)

# Generator Network
class Generator(nn.Module):
    def __init__(self, latent_dim=100, num_classes=10):
        super(Generator, self).__init__()
        self.latent_dim = latent_dim
        self.num_classes = num_classes

        # Label embedding
        self.label_emb = nn.Embedding(num_classes, num_classes)

        # Generator layers
        self.main = nn.Sequential(
            # Input: latent_dim + num_classes
            nn.Linear(latent_dim + num_classes, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),

            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),

            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(1024),

            nn.Linear(1024, img_size * img_size),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        # Embed labels and concatenate with noise
        label_emb = self.label_emb(labels)
        gen_input = torch.cat([noise, label_emb], dim=1)

        # Generate image
        img = self.main(gen_input)
        img = img.view(img.size(0), 1, img_size, img_size)
        return img

# Discriminator Network
class Discriminator(nn.Module):
    def __init__(self, num_classes=10):
        super(Discriminator, self).__init__()
        self.num_classes = num_classes

        # Label embedding
        self.label_emb = nn.Embedding(num_classes, num_classes)

        # Discriminator layers
        self.main = nn.Sequential(
            # Input: img_size*img_size + num_classes
            nn.Linear(img_size * img_size + num_classes, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),

            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),

            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),

            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, img, labels):
        # Flatten image and embed labels
        img_flat = img.view(img.size(0), -1)
        label_emb = self.label_emb(labels)

        # Concatenate image and label embeddings
        disc_input = torch.cat([img_flat, label_emb], dim=1)

        # Get validity score
        validity = self.main(disc_input)
        return validity

# Initialize models
generator = Generator(latent_dim, num_classes=10).to(device)
discriminator = Discriminator(num_classes=10).to(device)

# Loss function
adversarial_loss = nn.BCELoss()

# Optimizers
optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))

# Training function
def train_gan():
    generator.train()
    discriminator.train()

    for epoch in range(num_epochs):
        for i, (real_imgs, labels) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
            batch_size = real_imgs.size(0)

            # Move to device
            real_imgs = real_imgs.to(device)
            labels = labels.to(device)

            # Create labels for adversarial loss
            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)

            # ---------------------
            #  Train Discriminator
            # ---------------------
            optimizer_D.zero_grad()

            # Real images
            real_validity = discriminator(real_imgs, labels)
            d_real_loss = adversarial_loss(real_validity, real_labels)

            # Fake images
            noise = torch.randn(batch_size, latent_dim).to(device)
            fake_labels_input = torch.randint(0, 10, (batch_size,)).to(device)
            fake_imgs = generator(noise, fake_labels_input)
            fake_validity = discriminator(fake_imgs.detach(), fake_labels_input)
            d_fake_loss = adversarial_loss(fake_validity, fake_labels)

            # Total discriminator loss
            d_loss = (d_real_loss + d_fake_loss) / 2
            d_loss.backward()
            optimizer_D.step()

            # -----------------
            #  Train Generator
            # -----------------
            optimizer_G.zero_grad()

            # Generate fake images
            noise = torch.randn(batch_size, latent_dim).to(device)
            fake_labels_input = torch.randint(0, 10, (batch_size,)).to(device)
            fake_imgs = generator(noise, fake_labels_input)

            # Generator wants discriminator to classify fake images as real
            fake_validity = discriminator(fake_imgs, fake_labels_input)
            g_loss = adversarial_loss(fake_validity, real_labels)

            g_loss.backward()
            optimizer_G.step()

            # Print losses occasionally
            if i % 200 == 0:
                print(f"[Epoch {epoch+1}/{num_epochs}] [Batch {i}/{len(train_loader)}] "
                      f"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]")

        # Save sample images every 10 epochs
        if (epoch + 1) % 10 == 0:
            save_sample_images(epoch + 1)

    # Save final model
    torch.save(generator.state_dict(), 'generator.pth')
    torch.save(discriminator.state_dict(), 'discriminator.pth')
    print("Training completed! Models saved.")

def save_sample_images(epoch):
    """Save sample images for each digit"""
    generator.eval()
    with torch.no_grad():
        # Generate one image for each digit
        noise = torch.randn(10, latent_dim).to(device)
        labels = torch.arange(0, 10).to(device)
        fake_imgs = generator(noise, labels)

        # Denormalize images
        fake_imgs = fake_imgs * 0.5 + 0.5

        # Create subplot
        fig, axes = plt.subplots(2, 5, figsize=(10, 4))
        for i in range(10):
            row = i // 5
            col = i % 5
            axes[row, col].imshow(fake_imgs[i].cpu().squeeze(), cmap='gray')
            axes[row, col].set_title(f'Digit {i}')
            axes[row, col].axis('off')

        plt.suptitle(f'Generated Images - Epoch {epoch}')
        plt.tight_layout()
        plt.savefig(f'generated_samples_epoch_{epoch}.png')
        plt.show()

    generator.train()

# Function to generate specific digits (for the web app)
def generate_digit_images(digit, num_images=5):
    """Generate multiple images of a specific digit"""
    generator.eval()
    with torch.no_grad():
        noise = torch.randn(num_images, latent_dim).to(device)
        labels = torch.full((num_images,), digit, dtype=torch.long).to(device)
        fake_imgs = generator(noise, labels)

        # Denormalize images
        fake_imgs = fake_imgs * 0.5 + 0.5
        fake_imgs = torch.clamp(fake_imgs, 0, 1)

        return fake_imgs.cpu().numpy()

# Start training
if __name__ == "__main__":
    print("Starting GAN training...")
    train_gan()

    # Test generation after training
    print("\nTesting digit generation...")
    for digit in range(10):
        images = generate_digit_images(digit, 5)
        print(f"Generated {images.shape[0]} images for digit {digit}")

    print("\nTraining script completed!")
    print("Upload the 'generator.pth' file to your web app deployment.")